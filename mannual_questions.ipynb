{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, re\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "import openai\n",
    "\n",
    "def get_openai_response(request, exam_marks=0):\n",
    "    # random api key from few keys\n",
    "    # openai.api_key = os.getenv(random.choice([\"OPENAI_API_KEY\", \"OPENAI_API_KEY1\", \"OPENAI_API_KEY2\"]))\n",
    "    # openai.api_key = os.getenv(\"test_key\")\n",
    "    openai.api_key = \"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\"\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        # model = \"gpt-3.5-turbo\",\n",
    "        prompt=request,\n",
    "        temperature=0.3,  # 0 temperature controls the randomness of predictions. 0 means only the most likely word is predicted. 1 means picking a word is completely random. We generally recommend using values between 0.9 and 1.2.\n",
    "        max_tokens=1000,    # 1000 max_tokens controls the maximum number of tokens to generate. Requests can use up to 2048 tokens shared between prompt and completion. (One token is roughly 4 characters for normal English text)\n",
    "        top_p=0.2,    # 1 top_p controls diversity via nucleus sampling: 0.0 = no diversity, 1.0 = maximum diversity\n",
    "        frequency_penalty=0.2,    # 0 frequency_penalty adjusts how much the model favors repeating existing lines\n",
    "        presence_penalty=0.2      # 0.3 presence_penalty adjusts how much the model favors generating new lines rather than repeating existing ones\n",
    "    )\n",
    "    \n",
    "    # response = openai_response_with_backoff(model=\"text-davinci-002\", prompt=request, temperature=0, max_tokens=1000, top_p=1, frequency_penalty=0, presence_penalty=0)\n",
    "    \n",
    "    print(response)\n",
    "    # print(1)\n",
    "    return response['choices'][0]['text']\n",
    "\n",
    "\n",
    "# get_openai_response('Define centers of suspension and oscillation of a compound pendulum')\n",
    "# get_openai_response('Explain how single wire can radiate electromagnetic waves with necessary radiation patterns and conditions.')\n",
    "\n",
    "\n",
    "def is_valid_question(question):\n",
    "    is_valid = True\n",
    "    # remove all special characters and numeric characters\n",
    "    question = re.sub('[^A-Za-z]+', ' ', question).replace('\\n', '').replace('\\t','').replace('\\r','').replace('  ','')\n",
    "    if question == '' or len(question) < 8:\n",
    "        is_valid = False\n",
    "    return is_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mfor\u001b[39;00m question, mark \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(question_list, marks_list):\n\u001b[1;32m     46\u001b[0m     \u001b[39mif\u001b[39;00m is_valid_question(question):\n\u001b[0;32m---> 47\u001b[0m         answer_list\u001b[39m.\u001b[39mappend(get_openai_response(question, mark))\n\u001b[1;32m     48\u001b[0m         \u001b[39mif\u001b[39;00m answer_list \u001b[39m!=\u001b[39m []:\n\u001b[1;32m     49\u001b[0m             \u001b[39m# store answer in json\u001b[39;00m\n\u001b[1;32m     50\u001b[0m             \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file_to_read_write,\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m, in \u001b[0;36mget_openai_response\u001b[0;34m(request, exam_marks)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_openai_response\u001b[39m(request, exam_marks\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[39m# random api key from few keys\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[39m# openai.api_key = os.getenv(random.choice([\"OPENAI_API_KEY\", \"OPENAI_API_KEY1\", \"OPENAI_API_KEY2\"]))\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[39m# openai.api_key = os.getenv(\"test_key\")\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     openai\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msk-boyoZ8c4FyvIdfBLIzsmT3BlbkFJAQK8I12IUTcEgqKZvHax\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     13\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-davinci-003\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m         \u001b[39m# model = \"gpt-3.5-turbo\",\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m         prompt\u001b[39m=\u001b[39;49mrequest,\n\u001b[1;32m     16\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m,  \u001b[39m# 0 temperature controls the randomness of predictions. 0 means only the most likely word is predicted. 1 means picking a word is completely random. We generally recommend using values between 0.9 and 1.2.\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m         max_tokens\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,    \u001b[39m# 1000 max_tokens controls the maximum number of tokens to generate. Requests can use up to 2048 tokens shared between prompt and completion. (One token is roughly 4 characters for normal English text)\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m         top_p\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,    \u001b[39m# 1 top_p controls diversity via nucleus sampling: 0.0 = no diversity, 1.0 = maximum diversity\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,    \u001b[39m# 0 frequency_penalty adjusts how much the model favors repeating existing lines\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m      \u001b[39m# 0.3 presence_penalty adjusts how much the model favors generating new lines rather than repeating existing ones\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     23\u001b[0m     \u001b[39m# response = openai_response_with_backoff(model=\"text-davinci-002\", prompt=request, temperature=0, max_tokens=1000, top_p=1, frequency_penalty=0, presence_penalty=0)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[39mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/Documents/machine_env/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/Documents/machine_env/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/Documents/machine_env/lib/python3.10/site-packages/openai/api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/Documents/machine_env/lib/python3.10/site-packages/openai/api_requestor.py:619\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    612\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    613\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    616\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 619\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    620\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    623\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    624\u001b[0m         ),\n\u001b[1;32m    625\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    626\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/machine_env/lib/python3.10/site-packages/openai/api_requestor.py:682\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    680\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    681\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    683\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "# check if file exist\n",
    "# file_to_read_write = 'questions2.json'\n",
    "# if not os.path.exists(file_to_read_write):\n",
    "file_to_read_write = 'mannual_questions.json'\n",
    "# file_to_read_write = 'past does haunt/data.json'\n",
    "\n",
    "# import questions\n",
    "with open(file_to_read_write,'r') as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "\n",
    "# propagation and antenna questions\n",
    "# antenna = [question for question in questions if question['subject'] == 'Propagation and Antenna']\n",
    "antenna = questions.copy()\n",
    "\n",
    "# # fetch answer from openai api\n",
    "# def get_answers(question, marks):\n",
    "#     # print(question, marks)\n",
    "#     print('answered')\n",
    "#     return 'answered'\n",
    "#     # answers = []\n",
    "#     # for q, m in zip(question, marks):\n",
    "#     #     if m == 1:\n",
    "#     #         answers.append(q)\n",
    "#     # return ans\"\\n\\nPositive logic is a type of logic where a logic level of 1 is considered to be true and a logic level of 0 is considered to be false. Negative logic is a type of logic where a logic level of 0 is considered to be true and a logic level of 1 is considered to be false.\\n\\nPositive X-OR is a logic gate that outputs a logic level of 1 when either of its two inputs is 1, and a logic level of 0 when both of its inputs are 0. Negative X-NOR is a logic gate that outputs a logic level of 0 when either of its two inputs is 0, and a logic level of 1 when both of its inputs are 1.\\n\\nThe following truth table shows that positive X-OR is equivalent to negative X-NOR:\\n\\nInput A | Input B | Positive X-OR | Negative X-NOR\\n0 | 0 | 0 | 1\\n0 | 1 | 1 | 0\\n1 | 0 | 1 | 0\\n1 | 1 | 0 | 1\\n\\nTherefore, it can be seen that positive X-OR is equivalent to negative X-NOR.\"\n",
    "for question_by_year in antenna:\n",
    "    for i in question_by_year['questions']:\n",
    "        # antenna[0]['questions']['1']['marks']\n",
    "        question_list = question_by_year['questions'][i]['question']\n",
    "        if 'marks' not in question_by_year['questions'][i].keys():\n",
    "            print(question_by_year['questions'][i])\n",
    "            # exit code\n",
    "            import sys\n",
    "            sys.exit()\n",
    "        else:\n",
    "            marks_list = question_by_year['questions'][i]['marks']\n",
    "\n",
    "        # print(question_list['question'])\n",
    "        answer_list = []\n",
    "\n",
    "        # answer only if not answered\n",
    "        if 'answer' not in question_by_year['questions'][i].keys() or question_by_year['questions'][i]['answer'] == []:\n",
    "            for question, mark in zip(question_list, marks_list):\n",
    "                if is_valid_question(question):\n",
    "                    answer_list.append(get_openai_response(question, mark))\n",
    "                    if answer_list != []:\n",
    "                        # store answer in json\n",
    "                        with open(file_to_read_write,'r') as f:\n",
    "                            questions_copy = json.load(f)\n",
    "                        \n",
    "                        # antenna_copy = [question for question in questions if question['subject'] == 'Propagation and Antenna']\n",
    "                        questions_copy[questions.index(question_by_year)]['questions'][i]['answer'] = answer_list\n",
    "                        # print(questions_copy)\n",
    "                        with open(file_to_read_write,'w') as f2:\n",
    "                            json.dump(questions_copy, f2, indent=4)\n",
    "\n",
    "# print(antenna_copy)\n",
    "'''\n",
    "\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "rw_file = 'mannual_questions.json'\n",
    "rw_file = 'past does haunt/data.json'\n",
    "with open(rw_file, 'r') as file:\n",
    "    questions = json.load(file)\n",
    "\n",
    "for question_paper in questions:\n",
    "    for question in question_paper['questions']:\n",
    "        # print(question_paper['questions'][question].keys())\n",
    "        if 'marks' not in question_paper['questions'][question].keys():\n",
    "            print(f\"\\n no marks in question: {question_paper['questions'][question]},\\n keys: {question_paper['questions'][question].keys()}\\n has_marks: {'marks' in question_paper['questions'][question].keys()}\")\n",
    "        # print(question)\n",
    "        # if 'marks' not in question_paper['questions'][question].keys():\n",
    "        #     print(question_paper['questions'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Questions and Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mannual_questions.json') as file:\n",
    "    questions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os, random\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"test\")\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"Explain how single wire can radiate electromagnetic waves with necessary radiation patterns and conditions.\\n\\nSingle wire radiators are simple antennas that are commonly used in radio systems. They are easy to construct and can be used to radiate electromagnetic waves with a variety of radiation patterns. The radiation pattern of a single wire radiator depends on its length and shape, and the frequency of the electromagnetic wave being radiated. \\n\\nAt lower frequencies, a single wire radiator can be used to produce omnidirectional radiation patterns, in which the antenna radiates evenly in all directions. This is accomplished by making the antenna a quarter-wavelength long, which is determined by the formula \\n\\nwavelength = (speed of light/frequency). \\n\\nAt higher frequencies, a single wire radiator can be used to generate directional radiation patterns. To produce a directional radiation pattern, the antenna must be longer than a quarter-wavelength. The radiation pattern will depend on the length of the antenna, the shape of the antenna, and the frequency of the electromagnetic wave being radiated. \\n\\nIn addition to its size and shape, the single wire radiator must also be properly grounded in order to create the desired radiation pattern. If the antenna is not properly grounded, the radiation pattern may be distorted or weakened. \\n\\nIn summary, single wire radiators can\",\n",
    "  temperature=0.7,\n",
    "  max_tokens=256,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0\n",
    ")\n",
    "\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
